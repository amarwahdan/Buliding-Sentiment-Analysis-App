# -*- coding: utf-8 -*-
"""Buliding Sentiment Analysis App

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ks92jW6wDNt2GaEwo9jnGNAElWhsZP2P

**buliding AI App from Scratch *and*  Buliding AI App
for Hugging Face**

***------------------------------------------------------------------------------------------------------------------------------------------------------------***

# Buliding Sentiment Analysis App *Project 1*

**Introduction**

*The project is a sentiment analysis application utilizing the Sentiment140 dataset, which contains 1.6 million tweets. The objective is to classify tweets as positive or negative using the Multinomial Naive Bayes classification model. The project involves text preprocessing, feature extraction using TF-IDF, and training the model to achieve improved accuracy. Results are presented through an interactive Gradio interface that allows users to input a tweet and determine its sentiment. The project focuses on enhancing performance by tuning hyperparameters and optimizing data preprocessing* . Data Source: https://www.kaggle.com/datasets/kazanova/sentiment140


***Amar*** ***Ahmed***

**Data scientist** | **Machine Learning Developer** | **Computer Vision Researcher**
"""

import pandas as pd
import kagglehub
import os

# Download latest version
path = kagglehub.dataset_download("kazanova/sentiment140")

file_path = os.path.join(path, 'training.1600000.processed.noemoticon.csv')

print("Path to dataset files:", file_path)
columns = ['target', 'ids', 'date', 'flag', 'user', 'text']
Data = pd.read_csv(file_path, encoding='latin-1', names=columns)
Data = Data.drop(columns=['ids','flag','user'])
Data

!pip install gradio

!pip install ydata_profiling

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.utils import shuffle
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix, classification_report, precision_score
from ydata_profiling import ProfileReport
# Data
Sentiment_data = Data
sample_data = 20000
Sentiment_data = Sentiment_data.sample(n=sample_data, random_state=42)
Sentiment_data.head()

Sentiment_data.tail()

"""# EDA"""

# Data Overview
display("Data size:", Sentiment_data.shape)
print('--------------------------------------------')
display("\n columns:", Sentiment_data.columns.tolist())
print('--------------------------------------------')
display("\n Data type:", Sentiment_data.dtypes)
print('--------------------------------------------')

Sentiment_data.info()

Sentiment_data.isnull().sum().sum()

Sentiment_data.duplicated().sum()

# Descriptive analysis
Sentiment_data.describe()

Sentiment_data.describe(include='object')

Sentiment_data['text'].value_counts()

Sentiment_data['target'].value_counts()

# Full Report
ProfileReport(Sentiment_data)

sentiment_counts = Sentiment_data['target'].value_counts()
plt.figure(figsize=(8, 6))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)
plt.title('Distribution of feelings')
plt.show()

"""**Data clenning & Data preprocessing**


"""

def Data_clenning():
    global Sentiment_data
    Sentiment_data['date'] = pd.to_datetime(Sentiment_data['date'])
    # Drop Duplicated Value
    Sentiment_data = Sentiment_data.drop_duplicates()
    return Sentiment_data
Data_clenning()

Sentiment_data.duplicated().sum()

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')


def clean_text(text):
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    # Remove user mentions
    text = re.sub(r'@\w+', '', text)
    # Remove hashtags
    text = re.sub(r'#\w+', '', text)
    # Remove special characters and punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Convert to lowercase
    text = text.lower()
    # Remove stop words
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(text)
    text = ' '.join([word for word in tokens if word not in stop_words])
    return text

Sentiment_data['cleaned_text'] = Sentiment_data['text'].apply(clean_text)

Sentiment_data['cleaned_text']

"""# Data Visualization Whis *WordCloud PLOT*"""

from wordcloud import WordCloud
Sentiment_data['clean_text'] = Sentiment_data['text'].apply(clean_text)

positive_text = ' '.join(Sentiment_data[Sentiment_data['target'] == 4]['clean_text'])
negative_text = ' '.join(Sentiment_data[Sentiment_data['target'] == 0]['clean_text'])

from wordcloud import WordCloud
plt.figure(figsize=(10, 6))
wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate(positive_text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Common words in positive tweets')
plt.show()

plt.figure(figsize=(10, 6))
wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate(negative_text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Common words in negative tweets')
plt.show()

"""# Spliting Dataset"""

X = Sentiment_data.drop(['text','date', 'target'], axis=1)
Y = Sentiment_data['target']

# TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=5, max_df=0.8)
X = vectorizer.fit_transform(X['cleaned_text'])

# shuffling
X, Y = shuffle(X, Y, random_state=42)

# spliting
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
display("X_train shape:", X_train.shape)
display("X_test shape:", X_test.shape)
display("Y_train shape:", Y_train.shape)
display("Y_test shape:", Y_test.shape)

"""# Train Model"""

import joblib
# Train Model
naivebayes_Model = MultinomialNB()
naivebayes_Model.fit(X_train, Y_train)

# Evaluate Model
train_score = naivebayes_Model.score(X_train, Y_train)
test_score = naivebayes_Model.score(X_test, Y_test)
print("Training Accuracy:", train_score)
print("Testing Accuracy:", test_score)
# Save Model
joblib.dump(vectorizer, 'vectorizer.pkl')
joblib.dump(naivebayes_Model, 'naivebayes_Model.pkl')

"""# cross validate Tools"""

from sklearn.model_selection import cross_validate, cross_val_score

# Model Check
cross_validate = cross_validate(naivebayes_Model,X, Y, cv=5)
display(cross_validate)
print('--------------------------------------------')
cross_val_score = cross_val_score(naivebayes_Model,X, Y, cv=5)
display(cross_val_score)

"""# metrics Test"""

# Predict Model
y_pred = naivebayes_Model.predict(X_test)
display("Predicted labels:", y_pred[0:5])
print('--------------------------------------------')
display("True labels:", Y_test[0:5])

confusion_matrix(Y_test, y_pred)

sns.heatmap(confusion_matrix(Y_test, y_pred), annot=True, fmt='d')
plt.title('Confusion matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()

from sklearn.metrics import classification_report
Report  = classification_report(Y_test, y_pred)
print(Report)

"""# Building Application for gradio"""

import gradio as gr
import joblib
def predict(text):
    try:
        t = clean_text(text)
        if not t:
            return "Error: Empty text after cleaning"

        vectorizer = joblib.load('vectorizer.pkl')
        naiveModel = joblib.load('naivebayes_Model.pkl')

        text_vec = vectorizer.transform([t])

        res = naiveModel.predict(text_vec)[0]

        if res == 4:
            return "Positive"
        else:
            return "Negative"
    except Exception as e:
        return f"Error in predict: {str(e)}"

# Launch Gradio interface
gr.Interface(fn=predict, inputs="text", outputs="text").launch()

"""***-------------------------------------------------------------------------------------------------------------------------------------------------------------***

# Buliding Sentiment Analysis Application Used Hugging Face *project 2*
"""

from transformers import pipeline

text_classifer = pipeline("text-classification", model="tabularisai/multilingual-sentiment-analysis")

text = 'I Love You Computer Vision'
respone = text_classifer(text) # result
print(respone)

import gradio as gr
def predict(text):
  respone = text_classifer(text)[0]
  return respone['label'], respone['score']

gr.Interface(fn=predict, inputs="text", outputs=["text", "number"]).launch()

"""# *Thank you for checking out this notebook❤️❤️!*"""